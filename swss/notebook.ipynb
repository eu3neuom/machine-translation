{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from lxml import etree\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from PyMultiDictionary import MultiDictionary\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "First, we start by parsing the trees generated using UCCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_words(father):\n",
    "    words = {}\n",
    "    for node in father:\n",
    "        if node.tag == \"node\":\n",
    "            for attribute in node:\n",
    "                words[node.attrib[\"ID\"]] = attribute.attrib[\"text\"]\n",
    "    return words\n",
    "\n",
    "def is_edge_remote(edge):\n",
    "    for attribute in edge:\n",
    "        if attribute.tag == \"attributes\" and \"remote\" in attribute.attrib.keys():\n",
    "            return attribute.attrib[\"remote\"]\n",
    "    return False\n",
    "\n",
    "def parse_edges(father):\n",
    "    edges = {}\n",
    "    for node in father:\n",
    "        if node.tag == \"node\":\n",
    "            id = node.attrib[\"ID\"]\n",
    "            for edge in node:\n",
    "                if edge.tag == \"edge\":\n",
    "                    toid = edge.attrib[\"toID\"]\n",
    "\n",
    "                    if is_edge_remote(edge):\n",
    "                        continue\n",
    "\n",
    "                    for category in edge:\n",
    "                        if category.tag == \"category\":\n",
    "                            tag = category.attrib[\"tag\"]\n",
    "                            edges[toid] = (id, tag)\n",
    "    return edges\n",
    "\n",
    "def parse_trees(trees_path, sentences_path, offset):\n",
    "    sentences = None\n",
    "    with open(sentences_path, \"r\") as file:\n",
    "        sentences = [line.strip(\"\\n\") for line in file.readlines() if line != \"\\n\"]\n",
    "\n",
    "    trees = {}\n",
    "    tree_files = os.listdir(trees_path)\n",
    "    tree_files.sort(key=lambda x: int(x[offset:-4]))\n",
    "    for i, file in enumerate(tree_files):\n",
    "        tree_path = os.path.join(trees_path, file)\n",
    "\n",
    "        parser = etree.XMLParser(recover=True, encoding=\"utf-8\")\n",
    "        tree = etree.parse(tree_path, parser=parser)\n",
    "\n",
    "        words = {}\n",
    "        edges = {}\n",
    "        for child in tree.getroot():\n",
    "            if child.tag == \"layer\" and child.attrib[\"layerID\"] == \"0\":\n",
    "                words = parse_words(child)\n",
    "            if child.tag == \"layer\" and child.attrib[\"layerID\"] == \"1\":\n",
    "                edges = parse_edges(child)\n",
    "        trees[sentences[i]] = (words, edges)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate the core-words for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_core_words(trees):\n",
    "    core_words_dict = {}\n",
    "    for sentence, (words, edges) in trees.items():\n",
    "        core_words = []\n",
    "        for node, (parent, _) in edges.items():\n",
    "            if node[0] == \"0\": # node is a leaf\n",
    "                _, tag = edges[parent]\n",
    "                if tag in \"PSAC\":\n",
    "                    core_words.append(words[node])\n",
    "        core_words_dict[sentence] = core_words\n",
    "    return core_words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each sentence we calculate aditional information such as:\n",
    "- the number of scenes \n",
    "- the number of nodes\n",
    "- the number of critical edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_penalty_information(trees):\n",
    "    penalties_dict = {}\n",
    "    for sentence, (words, edges) in trees.items():\n",
    "        scene_count = 0\n",
    "        for _, tag in edges.values():\n",
    "            if tag in \"PS\":\n",
    "                scene_count += 1\n",
    "\n",
    "        node_count = len(edges) - len(words) + 1\n",
    "\n",
    "        critical_edge_count = 0\n",
    "        for _, tag in edges.values():\n",
    "            if tag in \"PSA\":\n",
    "                critical_edge_count += 1\n",
    "\n",
    "        penalties_dict[sentence] = (scene_count, node_count, critical_edge_count)\n",
    "    return penalties_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of constant values, used for language control (\"en\" or \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"en\"\n",
    "REFERENCE_TREES = f\"../data/{LANGUAGE}/{LANGUAGE}-references-trees/\"\n",
    "REFERENCE_SENTENCES = f\"../data/{LANGUAGE}/{LANGUAGE}_refs.txt\"\n",
    "REFERENCE_OFFSET = 8\n",
    "SYSOUT_TREES = f\"../data/{LANGUAGE}/{LANGUAGE}-sysout-trees/\"\n",
    "SYSOUT_SENTENCES = f\"../data/{LANGUAGE}/{LANGUAGE}_sysout.txt\"\n",
    "SYSOUT_OFFSET = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for calculating the precision, recall and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OMEGA = 0.5\n",
    "\n",
    "def generate_synonims(word, stemmer):\n",
    "    synonims = []\n",
    "    if LANGUAGE == \"en\":\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                synonims.append(l.name())\n",
    "    else:\n",
    "        dictionary = MultiDictionary()\n",
    "        try:\n",
    "            synonims = dictionary.synonym(\"de\", word)\n",
    "        except:\n",
    "            synonims = []\n",
    "\n",
    "    synonims = [stemmer.stem(word) for word in synonims]\n",
    "    return set(synonims)\n",
    "\n",
    "def compute_precision_recall(reference, predict, stemmer, include_synonims):\n",
    "    matches = 0\n",
    "    marked = [False for _ in range(len(reference))]\n",
    "    for word_predict, stem_word_predict in predict:\n",
    "        match_found = False\n",
    "        for i, (_, stem_word_ref) in enumerate(reference):\n",
    "            if not marked[i] and stem_word_ref == stem_word_predict:\n",
    "                matches += 1\n",
    "                marked[i] = True\n",
    "                match_found = True\n",
    "                break\n",
    "        if not match_found and include_synonims:\n",
    "            synonims = generate_synonims(word_predict, stemmer)\n",
    "            for i, (_, stem_word_ref) in enumerate(reference):\n",
    "                if not marked[i] and stem_word_ref in synonims:\n",
    "                    matches += 1\n",
    "                    marked[i] = True\n",
    "                    break\n",
    "    precision = (matches / len(predict)) if len(predict) > 0 else OMEGA\n",
    "    recall = (matches / len(reference)) if len(reference) > 0 else OMEGA\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def calculate_accuracy(reference, predict, include_synonims):\n",
    "    if LANGUAGE == \"en\":\n",
    "        stemmer = PorterStemmer()\n",
    "    else:\n",
    "        stemmer = SnowballStemmer(\"german\")\n",
    "    \n",
    "    reference = list(zip(reference, [stemmer.stem(word) for word in reference]))\n",
    "    predict = list(zip(predict, [stemmer.stem(word) for word in predict]))\n",
    "\n",
    "    precision, recall = compute_precision_recall(reference, predict, stemmer, include_synonims)\n",
    "    f1 = (2.0 * precision * recall / (precision + recall)) if max(precision, recall) > 0 else 0.0\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penalties for a pair of (reference, predicted) sentences according to the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_penalties_for_pair(reference, predict):\n",
    "    # reference = (state_count, node_count, critical_edges_count)\n",
    "    # predict = (state_count, node_count, critical_edges_count)\n",
    "    ps = 0 if min(reference[0], predict[0]) == 0 else 1 - min(reference[0], predict[0]) / max(reference[0], predict[0])\n",
    "    pn = 0 if min(reference[1], predict[1]) == 0 else 1 - min(reference[1], predict[1]) / max(reference[1], predict[1])\n",
    "    pe = 0 if min(reference[2], predict[2]) == 0 else 1 - min(reference[2], predict[2]) / max(reference[2], predict[2])\n",
    "\n",
    "    return ps, pn, pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute all the training values for each pair of (reference, predict) sentences such as:\n",
    "- precision, recall, f1 score\n",
    "- penalties for states, nodes and critical edges\n",
    "- the average word count of a sentence pair\n",
    "- the bleu, meteor and rhcf scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "    return tokens\n",
    "    \n",
    "def count_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    return len(words)\n",
    "\n",
    "def generate_parameters(df, trees, include_synonims=False):\n",
    "    core_words = generate_core_words(trees)\n",
    "    penalties = generate_penalty_information(trees)\n",
    "\n",
    "    parameters = {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1\": [],\n",
    "        \"ps\": [],\n",
    "        \"pn\": [],\n",
    "        \"pe\": [],\n",
    "        \"Len\": [],\n",
    "        \"raw_score\": [],\n",
    "        \"bleu_score\": [],\n",
    "        \"meteor_score\": [],\n",
    "        \"chrf_score\": [],\n",
    "    }\n",
    "\n",
    "    bleu = BLEU(effective_order=True)\n",
    "    chrf = CHRF()\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        reference = row[\"segreference\"]\n",
    "        predict = row[\"segpredict\"]\n",
    "\n",
    "        precision, recall, f1 = calculate_accuracy(core_words[reference], core_words[predict], include_synonims)\n",
    "        parameters[\"precision\"].append(precision)\n",
    "        parameters[\"recall\"].append(recall)\n",
    "        parameters[\"f1\"].append(f1)\n",
    "\n",
    "        ps, pn, pe = calculate_penalties_for_pair(penalties[reference], penalties[predict])\n",
    "        parameters[\"ps\"].append(ps)\n",
    "        parameters[\"pn\"].append(pn)\n",
    "        parameters[\"pe\"].append(pe)\n",
    "\n",
    "        Len = (count_words(reference) + count_words(predict)) / 2.0\n",
    "        parameters[\"Len\"].append(Len)\n",
    "\n",
    "        parameters[\"raw_score\"].append(row[\"raw_score\"])\n",
    "        parameters[\"bleu_score\"].append(bleu.sentence_score(predict, [reference]).score)\n",
    "        parameters[\"meteor_score\"].append(single_meteor_score(custom_tokenizer(reference), custom_tokenizer(predict)))\n",
    "        parameters[\"chrf_score\"].append(chrf.sentence_score(predict, [reference]).score)\n",
    "\n",
    "    return pd.DataFrame(parameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    train_df = pd.read_csv(os.path.join(path, \"train.csv\")).dropna()\n",
    "    test_df = pd.read_csv(os.path.join(path, \"train.csv\")).dropna()\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df, test_df = load_dataset(f\"../data/{LANGUAGE}/\")\n",
    "\n",
    "trees = {\n",
    "    **parse_trees(REFERENCE_TREES, REFERENCE_SENTENCES, REFERENCE_OFFSET),\n",
    "    **parse_trees(SYSOUT_TREES, SYSOUT_SENTENCES, SYSOUT_OFFSET)\n",
    "}\n",
    "core_words = generate_core_words(trees)\n",
    "penalties = generate_penalty_information(trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "### Functions for evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_swss_augment(df, model, parameters):\n",
    "    alpha1, alpha2, alpha3, alpha4, beta = parameters\n",
    "    augmented_scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        f1 = float(row[\"f1\"])\n",
    "        ps = float(row[\"ps\"])\n",
    "        pn = float(row[\"pn\"])\n",
    "        pe = float(row[\"pe\"])\n",
    "        Len = float(row[\"Len\"])\n",
    "        model_score = float(row[f\"{model}_score\"])\n",
    "\n",
    "        score = f1 * math.exp(-alpha1 * ps - alpha2 * pn - alpha3 * pe - alpha4 * Len)\n",
    "        augmented_score = model_score + beta * score\n",
    "        augmented_scores.append(augmented_score)\n",
    "    return augmented_scores\n",
    "\n",
    "def evaluate(df, model, parameters):\n",
    "    model_correlation = np.corrcoef(df.raw_score, df[f\"{model}_score\"])[0][1]\n",
    "\n",
    "    augmented_scores = calculate_swss_augment(df, model, parameters)\n",
    "    model_swss_correlation = np.corrcoef(df.raw_score, augmented_scores)[0][1]\n",
    "\n",
    "    print(f\"{model} only score:   [{model_correlation}]\")\n",
    "    print(f\"{model} + swss score: [{model_swss_correlation}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "### Without synonims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu only score:   [0.34224190946694766]\n",
      "bleu + swss score: [0.34261679790904326]\n",
      "meteor only score:   [0.5289765358914779]\n",
      "meteor + swss score: [0.5312975958919443]\n",
      "chrf only score:   [0.5074722657623074]\n",
      "chrf + swss score: [0.507587844010924]\n"
     ]
    }
   ],
   "source": [
    "train_params = generate_parameters(train_df, trees, include_synonims=False)\n",
    "test_params = generate_parameters(test_df, trees, include_synonims=False)\n",
    "\n",
    "#alpha1, alpha2, alpha3, alpha4, beta\n",
    "parameters = [0.2, 1, 0.1, 0.01, 0.2]\n",
    "evaluate(test_params, \"bleu\", parameters)\n",
    "evaluate(test_params, \"meteor\", parameters)\n",
    "evaluate(test_params, \"chrf\", parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With synonims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu only score:   [0.34224190946694766]\n",
      "bleu + swss score: [0.342642984350138]\n",
      "meteor only score:   [0.5289765358914779]\n",
      "meteor + swss score: [0.5332413108644422]\n",
      "chrf only score:   [0.5074722657623074]\n",
      "chrf + swss score: [0.5076255010016812]\n"
     ]
    }
   ],
   "source": [
    "train_params = generate_parameters(train_df, trees, include_synonims=True)\n",
    "test_params = generate_parameters(test_df, trees, include_synonims=True)\n",
    "\n",
    "#alpha1, alpha2, alpha3, alpha4, beta\n",
    "parameters = [0.2, 1, 0.1, 0.01, 0.2]\n",
    "evaluate(test_params, \"bleu\", parameters)\n",
    "evaluate(test_params, \"meteor\", parameters)\n",
    "evaluate(test_params, \"chrf\", parameters)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c76e8eed2d26e1c6d9fdc92fd395049f4a77e86ba4da77f2022e3ef61133537f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
