{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52927cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cd4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(scores_path, doc_metrics_path):\n",
    "    df_scores = pd.read_csv(scores_path)\n",
    "    df_doc_metrics = pd.read_csv(doc_metrics_path, sep=\" \")\n",
    "    \n",
    "    score_columns = [\n",
    "        \"raw_score\", \"bleu_score\", \"meteor_score\", \"chrf_score\", \"monolingual_bert_score\", \"multilingual_bert_score\"\n",
    "    ]\n",
    "    \n",
    "    d_seg_scores = {}\n",
    "    for col in score_columns:\n",
    "        d_seg_scores[col] = {}\n",
    "    \n",
    "    # for each segment, store the maximum score computed for all references\n",
    "    for _, row in df_scores.iterrows():\n",
    "        for col in score_columns:\n",
    "            score = row[col]\n",
    "            segment = (row.sysid, row.docid, row.segid)\n",
    "            old = d_seg_scores[col].get(segment, 0)\n",
    "            new = max(old, score)\n",
    "            d_seg_scores[col][segment] = score\n",
    "            \n",
    "    all_segments = list(d_seg_scores[\"raw_score\"].keys())\n",
    "    print(\"Segment correlations:\")\n",
    "    all_raw_scores = [d_seg_scores[\"raw_score\"][segment] for segment in all_segments]\n",
    "    for col in score_columns:\n",
    "        all_col_scores = [d_seg_scores[col][segment] for segment in all_segments]\n",
    "        corr, _ = pearsonr(all_raw_scores, all_col_scores)\n",
    "        print(f\"\\t{col}: {corr}\")\n",
    "        \n",
    "    d_doc_acumulator = {}\n",
    "    for col in score_columns:\n",
    "        d_doc_acumulator[col] = {}\n",
    "        \n",
    "    # for each document, add the scores computed for each segment\n",
    "    for segment in all_segments:\n",
    "        for col in score_columns:\n",
    "            sysid, docid, _ = segment\n",
    "            document = (sysid, docid)\n",
    "            old = d_doc_acumulator[col].get(document, 0)\n",
    "            new = old + d_seg_scores[col][segment]\n",
    "            d_doc_acumulator[col][document] = new\n",
    "        \n",
    "    # divide accumulated scores for each document by the number of segments in the document\n",
    "    for _, row in df_doc_metrics.iterrows():\n",
    "        document = (row.SYS, row.SEGID)\n",
    "        if document not in d_doc_acumulator[\"raw_score\"]:\n",
    "            continue\n",
    "        d_doc_acumulator[\"raw_score\"][document] = row.N * row[\"RAW.SCR\"]\n",
    "        for col in score_columns:\n",
    "            d_doc_acumulator[col][document] /= row.N\n",
    "            \n",
    "    all_documents = list(d_doc_acumulator[\"raw_score\"].keys())\n",
    "    print(\"Document correlations:\")\n",
    "    all_raw_scores = [d_doc_acumulator[\"raw_score\"][document] for document in all_documents]\n",
    "    for col in score_columns:\n",
    "        all_col_scores = [d_doc_acumulator[col][document] for document in all_documents]\n",
    "        corr, _ = pearsonr(all_raw_scores, all_col_scores)\n",
    "        print(f\"\\t{col}: {corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9473ec",
   "metadata": {},
   "source": [
    "## German-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b68f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment correlations:\n",
      "\traw_score: 1.0\n",
      "\tbleu_score: 0.46319160375197915\n",
      "\tmeteor_score: 0.6416436857747883\n",
      "\tchrf_score: 0.6425058781474778\n",
      "\tmonolingual_bert_score: 0.7120993136320412\n",
      "\tmultilingual_bert_score: 0.6630953396480903\n",
      "Document correlations:\n",
      "\traw_score: 0.9999999999999999\n",
      "\tbleu_score: 0.6894479572807775\n",
      "\tmeteor_score: 0.7837446893852951\n",
      "\tchrf_score: 0.7699703682531935\n",
      "\tmonolingual_bert_score: 0.7946188830855865\n",
      "\tmultilingual_bert_score: 0.7706698141993786\n"
     ]
    }
   ],
   "source": [
    "compute_correlations(\"data/en-test-scores.csv\", \"data/metrics-ad-doc-scores-de-en.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c843425",
   "metadata": {},
   "source": [
    "## English-German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c9cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment correlations:\n",
      "\traw_score: 0.9999999999999997\n",
      "\tbleu_score: 0.28843973792229216\n",
      "\tmeteor_score: 0.3061951962832597\n",
      "\tchrf_score: 0.3944730948395032\n",
      "\tmonolingual_bert_score: 0.538809381700918\n",
      "\tmultilingual_bert_score: 0.36947858448621407\n",
      "Document correlations:\n",
      "\traw_score: 1.0\n",
      "\tbleu_score: 0.3993291815922161\n",
      "\tmeteor_score: 0.4314120932300453\n",
      "\tchrf_score: 0.5216788942053152\n",
      "\tmonolingual_bert_score: 0.68094289522945\n",
      "\tmultilingual_bert_score: 0.5157034823355668\n"
     ]
    }
   ],
   "source": [
    "compute_correlations(\"data/de-test-scores.csv\", \"data/metrics-ad-doc-scores-en-de.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt-metrics",
   "language": "python",
   "name": "mt-metrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
